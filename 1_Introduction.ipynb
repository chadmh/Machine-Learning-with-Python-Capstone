{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1. Introduction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOzodOqmAkEcq83psUmQ4bI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chadmh/Short-Hands-on-Tutorial-for-Deep-Learning-in-Tensorflow/blob/master/1_Introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13cu8b2pyOzR"
      },
      "source": [
        "# 1.1 A Minimal Deep Network with MNIST\n",
        "\n",
        "One of the classic machine learning problems is digit recognition using the MNIST dataset.  Various solutions can be found on the web, and this notebook loosely follows [a process described on the Tensorflow website](https://colab.research.google.com/github/tensorflow/datasets/blob/master/docs/keras_example.ipynb#scrollTo=nTFoji3INMEM).  MNIST contains 70000 images of the handwritten digits 0 - 9 along with the correct digit label for each image.  The dataset is widely available as a standard model testing benchmark and can be accessed within Tensorflow as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oC4E5rDnzAX2",
        "outputId": "ff8a9827-d9a7-4ef2-d52d-9a5e6cf01e74"
      },
      "source": [
        "# Import the needed tensorflow components\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Load the MNIST dataset.  Load checks whether the dataset is locally available and downloads it from \n",
        "# its official repository at http://yann.lecun.com/exdb/mnist if it cannot be found.\n",
        "(train, test), info = tfds.load('mnist',                  # Pick the MNIST dataset\n",
        "                                 split=['train', 'test'], # Load both the training and testing parts of the dataset\n",
        "                                 with_info=True,          # Generate summary information about the dataset\n",
        "                                 as_supervised=True)      # return both the inputs and labels as a tuple\n",
        "\n",
        "print(info.description)\n",
        "print(info.splits)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The MNIST database of handwritten digits.\n",
            "{'test': <tfds.core.SplitInfo num_examples=10000>, 'train': <tfds.core.SplitInfo num_examples=60000>}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cwff-r2V3SGv"
      },
      "source": [
        "As with any dataset, the data must be preprocessed prior to feeding it to whatever model will train on it.  In the case of MNIST, each input image is 28 pixels by 28 pixels with uint8 encoding (integer grayscale values from 0 to 255).  Each label, or, output answer is a numeric value between 0 and 9.\n",
        "\n",
        "train and test are [Tensorflow Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) objects.  Technically, Tensorflow does not attempt to load 70000 images at once but creates accessor objects that will iteratively load the needed images from disk as requested.  While this approach may seem overkill for a small dataset like MNIST, it is essential once the size of the dataset exceeds system memory.  Tensorflow is designed to efficiently process datasets that are terabytes or even petabytes in size.\n",
        "\n",
        "The next step, therefore, is to create the data preprocessing pipeline and assign it to the train and test objects.  The preprocessing steps will then be applied by Tensorflow whenever the data are loaded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBT-P-4q4HtT",
        "outputId": "f4d155dd-d198-4e1b-bf3b-543e9da5bf0d"
      },
      "source": [
        "# Define the data preprocessing pipeline.  For MNIST, the only needed preprocessing is to convert from unit8 to \n",
        "# float.  Other data sets are likely more extensive.\n",
        "def preprocess_data(input, label):\n",
        "  # Convert unit8 to real on [0, 1]\n",
        "  input = tf.cast(input, tf.float32) / 255.0\n",
        "  return input, label\n",
        "\n",
        "# Assign the preprocessing pipeline to each dataset: train and test\n",
        "train = train.map(preprocess_data)\n",
        "test = test.map(preprocess_data)\n",
        "\n",
        "# Tell each dataset how many images it will load at once for processing\n",
        "BATCH_SIZE=128\n",
        "train = train.batch(BATCH_SIZE)\n",
        "test = test.batch(BATCH_SIZE)\n",
        "\n",
        "train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((None, 28, 28, 1), (None,)), types: (tf.float32, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtX7-eYy9oit"
      },
      "source": [
        "Now that the datasets are set up for processing, it is time to design the model that will be trained.  For image problems like this, typically some form of convolutional neural network (CNN) is used; however, for this simple demonstration, let's just use a basic deep neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFucPVox4Gkh",
        "outputId": "fbd377d2-04bb-4efd-f6ce-d8da88c7eeba"
      },
      "source": [
        "# Specify a basic sequential neural network\n",
        "model = tf.keras.models.Sequential([\n",
        "                                   tf.keras.layers.Flatten(input_shape=(28, 28)), # Input Layer: Convert 28 x 28 image into 784 x 1 vector\n",
        "                                   tf.keras.layers.Dense(20, activation='relu'),  # Hidden Layer: Define a layer with 20 neurons\n",
        "                                   tf.keras.layers.Dense(10)                      # Output Layer: Define a layer with a slot for each digit.\n",
        "                                   ])\n",
        "\n",
        "# Define the optimizing algorithm and optimizing criteria for the model\n",
        "model.compile(\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),              # Use the Adam optimizer to train the model\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), # Minimize the categorical crossentropy function in training\n",
        "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]                # Measure our overall accuracy to see how well we've trained\n",
        "              )\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_20\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_20 (Flatten)         (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_40 (Dense)             (None, 20)                15700     \n",
            "_________________________________________________________________\n",
            "dense_41 (Dense)             (None, 10)                210       \n",
            "=================================================================\n",
            "Total params: 15,910\n",
            "Trainable params: 15,910\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayzHprVeDHOu"
      },
      "source": [
        "The above model is a three layer network with one input layer, one hidden layer, and one output layer.  The model is still simple enough to be mathematically represented with ease as\n",
        "\n",
        "> (1.1) *y = w2 * ( ReLU( w1 * x + b1 ) ) + b2*\n",
        "\n",
        "In this equation, x is a 784 x 1 vector, w1 is a 20 x 784 matrix, b1 is a 20 x 1 vector, w2 is a 10 x 20 matrix and b2 is a 10 x1 vector.  The output y is a 10 x 1 vector. Mathematically, there are 2 transformations connecting our 3 layers: A linear transformation with a nonlinear ReLU activation function that implements the input to hidden layer computation, and a second linear transformation that implements the hidden layer to output layer computation.  While neural networks are generally envisioned and designed in terms of artificial neuron layers, mathematically they are implemented as transformations between layers.\n",
        "\n",
        "`tf.keras.layers.Flatten` converts the 28 x 28 pixel image into a 784 x 1 column vector suitable for matrix multiplication.  The dense layer has 20 neurons with ReLU activation and therefore mandates a linear tranformation from 784 elements to 20 elements (w1 becomes a 20 x 784 matrix) followed by setting all negative elements to zero (the ReLU function).  The final layer has 10 outputs (w2 becomes a 10 x 20 matrix), or one output for each digit.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-SrDXeLD5jq",
        "outputId": "b4bf817d-34d2-49a2-c888-c03839cce97c"
      },
      "source": [
        "model.fit(train, epochs=2, validation_data=test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "469/469 [==============================] - 4s 7ms/step - loss: 0.6142 - sparse_categorical_accuracy: 0.8294 - val_loss: 0.3164 - val_sparse_categorical_accuracy: 0.9087\n",
            "Epoch 2/2\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.2979 - sparse_categorical_accuracy: 0.9146 - val_loss: 0.2637 - val_sparse_categorical_accuracy: 0.9231\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa4a9733990>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlGPlUiUDuz5"
      },
      "source": [
        "After two epochs (rounds of training on the data), the model has an estimated accuracy of 92%.  Increasing the number of epochs will increase the accuracy up to around 99%.  Further improvements can be made by using a convolutional model that accounts for the spatial relationships of the image pixels rather than flattening it into a vector.\n",
        "\n",
        "The above code only scratches the surface of deep neural networks, but in general, the process is similar:\n",
        "\n",
        "1.   Access the data\n",
        "2.   Clean and preprocess the data\n",
        "3.   Design the model, choose the optimizer, and select the minimization criteria\n",
        "4.   Fit the model to the data and evaluate performance.\n",
        "\n",
        "Deep learning provides an elegant mechanism for solving complex pattern recognition tasks with minimal programming.  This allows the machine learning developer to focus on the more creative tasks of data cleaning and model design."
      ]
    }
  ]
}